{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e330b0-f025-4c97-9af1-f8b12b4212f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007b8b79-13f6-4d62-97b6-b355952f524b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GpuDevice(id=0, process_index=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5eaa06f-ac05-481c-82d2-9b9eea783d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1e0ed-a8df-4180-956d-de51f3618ba8",
   "metadata": {},
   "source": [
    "## Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb1ddd-52f8-47e9-83db-085bcd613c21",
   "metadata": {},
   "source": [
    "#### Formulation in Population\n",
    "\n",
    "We are interested in offline imitation learning with an unknown return function $r(y) \\in \\{\\theta^T\\phi(y): \\theta\\in\\Theta\\}$ for $\\Theta:=\\{\\theta: \\geq 0, \\sum_k\\theta_k=1\\}$ with discrete treatment $t\\in\\mathcal{T}$.\n",
    "The expert policy depends on an unobserved confounder $U$ while the learned poicy does not, so that they are $\\pi_\\text{exp}(t|x, u)$ and $\\pi(t|x)$. Under these assumptions, the imitation learning (i.e. multiobjective policy improvement) from offline dataset becomes:\n",
    "$$\\hat\\pi \n",
    "= \\arg\\max_\\pi\\min_{\\theta\\in\\Theta}[\\min_{w\\in \\mathcal{W}}V_w^\\pi - V^{\\pi_\\text{exp}}].$$\n",
    "Here, the objective is defined as:\n",
    "$$\n",
    "\\min_{w\\in \\mathcal{W}}V_w^\\pi - V^{\\pi_\\text{exp}}\n",
    "= \\min_{w\\in\\mathcal{W}}\n",
    "\\mathbb{E}_X\\mathbb{E}_U\\left[\n",
    "\\sum_{t\\in\\mathcal{T}}\n",
    "  \\frac{\n",
    "    \\int\\mathrm{d}p(y|t, X, U)\\pi_\\text{exp}(t|X, U)w(t, X, U)\\pi(t|X) \\theta^T\\phi(y)\n",
    "  }{\n",
    "    \\mathbb{E}_X\\mathbb{E}_U[\\pi_\\text{exp}(t|X, U) w(t, X, U)]\n",
    "  }\n",
    "  - \\sum_{t\\in\\mathcal{T}}\\int\\mathrm{d}p(y|t, X, U)\\pi_\\text{exp}(t|X, U) \\theta^T\\phi(y) \n",
    "\\right],\n",
    "$$\n",
    "where $\\mathcal{W}$ is an uncertain set of the inverse treatment probability $\\frac{1}{\\pi_\\text{exp}(t|x, u)}$ satisfying $\\frac{1}{\\Gamma} \\leq \\frac{(1-\\pi_\\text{exp}(t|x))\\pi_\\text{exp}(t|x, u)}{\\pi_\\text{exp}(t|x)(1-\\pi_\\text{exp}(t|x, u))} \\leq \\Gamma$ for marginalized policy $\\pi_\\text{exp}(t|x) := \\mathbb{E}_U\\pi_\\text{exp}(t|x, U)$ (but $w$ does not necessarily satisfy the normalization requirements $\\sum_t \\frac{1}{w(t, x, u)}=1$ for any $x, u$).\n",
    "The first condition can be reformulated as \n",
    "$a_{t, x} \\leq w(t, x, u) \\leq b_{t, x}$ with $a_{t, x}:=1 + \\Gamma^{-1} \\cdot \\left(\\frac{1}{\\pi_\\text{exp}(t|x)}-1\\right)$ and $b_{t, x}:=1 + \\Gamma\\cdot\\left(\\frac{1}{\\pi_\\text{exp}(t|x)}-1\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64879eb2-0e36-43c6-9bfe-e8182b53bd1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Re-formulation in Sample \n",
    "Given samples $\\{(X_i, T_i, Y_i) \\}_{i=1}^n$ taken from the expert policy $\\pi_\\text{exp}$, we can approximate the above objective as follows:\n",
    "$$\n",
    "\\min_{w\\in \\mathcal{W}}V_w^\\pi - V^{\\pi_\\text{exp}}\n",
    "\\approx \\min_{w\\in\\mathcal{W}_n}\n",
    "\\sum_{t\\in\\mathcal{T}} \\sum_{i=1}^n \\left[\n",
    "  \\frac{\n",
    "    w_i\\pi(t|X_i) \\theta^T\\phi(Y_i)\\mathbb{1}[T_i=t]\n",
    "  }{\n",
    "    \\sum_{j=1}^n w_j  \\mathbb{1}[T_j=t]\n",
    "  }\n",
    "  - \\theta^T\\phi(Y_i)\\mathbb{1}[T_i=t]\n",
    "\\right],\n",
    "$$\n",
    "where \n",
    "$$\\mathcal{W}_n:= \\left\\{(w_1, \\ldots, w_n) :\n",
    "a_{T_i, X_i} \\leq w_i \\leq b_{T_i, X_i} \\text{ for any }i=1, \\ldots, n\n",
    "\\right\\}.\n",
    "$$\n",
    "Therefore, our imitation policy is given by\n",
    "$$\n",
    "\\hat \\pi = \n",
    "\\arg\\max_\\pi\\min_{\\theta\\in\\Theta}\\min_{w, \\psi}\n",
    "\\frac{1}{n}\\sum_{i=1}^n \\left[\n",
    "  w_i \\pi(T_i|X_i) \\theta^T\\phi(Y_i) - \\theta^T\\phi(Y_i) \n",
    "\\right]\n",
    "$$\n",
    "such that \n",
    "$$\n",
    "\\psi_t > 0 \\text{ and }\\sum_{i=1}^n w_i \\mathbb{1}[T_i=t] = 1 \\text{ for any }t\\in\\mathcal{T}\n",
    "\\text{ and }\\psi_{T_i} a_{T_i, X_i} \\leq w_i \\leq \\psi_{T_i} b_{T_i, X_i} \\text{ for any }i=1, \\ldots, n.\n",
    "$$\n",
    "Here, $\\psi_t$ represents the value of $\\frac{1}{\\sum_i w_i\\mathbb{1}[T_i=t]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81091716-5ac6-477c-9fc0-bc6668c241cc",
   "metadata": {},
   "source": [
    "## Multi-Objective Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e54ce6-518f-4b1b-817b-356565bf00d0",
   "metadata": {},
   "source": [
    "More details need to be added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170dfa7-bcb0-49d0-8013-ab4fce91762e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Re-formulation in Sample \n",
    "Given samples $\\{(X_i, T_i, Y_i) \\}_{i=1}^n$ taken from the expert policy $\\pi_\\text{exp}$, we can approximate the above objective as follows:\n",
    "$$\n",
    "\\min_{w\\in \\mathcal{W}} [V_w^\\pi - V_w^{\\pi_0}]\n",
    "\\approx \\min_{w\\in\\mathcal{W}_n}\n",
    "\\sum_{t\\in\\mathcal{T}} \\sum_{i=1}^n \\left[\n",
    "  \\frac{\n",
    "    w_i (\\pi(t|X_i) - \\pi_0(t| X_i)) \\theta^T\\phi(Y_i) \\mathbb{1}[T_j=t]\n",
    "  }{\n",
    "    \\sum_{j=1}^n w_j  \\mathbb{1}[T_j=t]\n",
    "  }\n",
    "\\right],\n",
    "$$\n",
    "where \n",
    "$$\\mathcal{W}_n:= \\left\\{(w_1, \\ldots, w_n) :\n",
    "a_{T_i, X_i} \\leq w_i \\leq b_{T_i, X_i} \\text{ for any }i=1, \\ldots, n\n",
    "\\right\\}.\n",
    "$$\n",
    "Therefore, our imitation policy is given by\n",
    "$$\n",
    "\\hat \\pi = \n",
    "\\arg\\max_\\pi\\min_{\\theta\\in\\Theta}\\min_{w, \\psi}\n",
    "\\frac{1}{n}\\sum_{i=1}^n \\left[\n",
    "  w_i (\\pi(T_i|X_i) - \\pi_0(T_i|X_i)) \\theta^T\\phi(Y_i)\n",
    "\\right]\n",
    "$$\n",
    "such that \n",
    "$$\n",
    "\\psi_t > 0 \\text{ and }\\sum_{i=1}^n w_i \\mathbb{1}[T_i=t] = 1 \\text{ for any }t\\in\\mathcal{T}\n",
    "\\text{ and }\\psi_{T_i} a_{T_i, X_i} \\leq w_i \\leq \\psi_{T_i} b_{T_i, X_i} \\text{ for any }i=1, \\ldots, n.\n",
    "$$\n",
    "Here, $\\psi_t$ represents the value of $\\frac{1}{\\sum_i w_i\\mathbb{1}[T_i=t]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934e0ed-e5ca-48fb-b916-4af20f239ea8",
   "metadata": {},
   "source": [
    "## Toy Example\n",
    "We consider the following 2-dimensional model:\n",
    "\\begin{align*}\n",
    "X_i&\\sim N(0, I_2)\n",
    "\\\\\n",
    "U_i&\\sim N(0, I_2)\n",
    "\\\\\n",
    "T_i &\\sim \\pi_\\text{exp}(t|X_i, U_i)\n",
    "\\\\\n",
    "Y_i&\\sim N(X_i + U_i + T_i, I_2)\n",
    "\\end{align*}\n",
    "where $t =(t_1, t_2)^T \\in \\mathcal{T}:=\\{-1, 0, 1\\}^2$. As for the class of return function, we define $\\phi(y):= (- \\|y_1\\|, - \\|y_2\\|)^T$ and assume the true parameter is $\\theta^*:=(0, 0)^T$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6470d78c-2949-4ad4-a7fe-c262f5b5bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_func(Y):\n",
    "    return -np.linalg.norm(Y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec20830a-fdcf-4100-94f7-568919ff939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n=1, pi=lambda x, u: np.random.choice((-1, 0, 1), 2 * x.shape[0]).reshape(x.shape)):\n",
    "    X = np.random.randn(n * 2).reshape(n, 2)\n",
    "    U = np.random.randn(n * 2).reshape(n, 2)\n",
    "    T = pi(X, U)\n",
    "    Y = X + U + T + np.random.randn(n * 2).reshape(n, 2)\n",
    "    return X, U, T, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99f1b517-c5b1-411a-8361-03faca0f7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, U, T, Y = generate_data(1000)\n",
    "R = return_func(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf0808-6ee2-41db-953b-d4888bd312fe",
   "metadata": {},
   "source": [
    "def plot_sequence(*args):\n",
    "    xy_prev = None\n",
    "    for xy in args:\n",
    "        plt.scatter(xy[:, 0], xy[:, 1], color='b')\n",
    "        if \n",
    "        plt.xlim(-3, 3)\n",
    "        plt.ylim(-3, 3)\n",
    "        xy_prev.append( = xy\n",
    "        \n",
    "def plot(X, U, T, Y):\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], alpha=0.1)\n",
    "    plt.scatter(U[:, 0], U[:, 1])\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(T[:, 0], T[:, 1])\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(Y[:, 0], Y[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b715b-018b-4143-8205-8c2310682e6a",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(9, 8))\n",
    "plot(X, U, T, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5accebb3-c8d4-491a-b22d-480d674381bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.380767529128187"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy value of random policy\n",
    "R.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3244ec-1990-4a4c-94a6-2933e962a111",
   "metadata": {},
   "source": [
    "### Multi-Objective Policy Improvement\n",
    "$\\pi_0(t|x) := - \\text{sign}(x) \\cdot \\mathbb{1}[\\|x\\|>0.5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d87fc1-1505-4d62-8fb9-c2a7b785fd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "643dd38f-963b-4c65-beaf-ca42c39cccf4",
   "metadata": {},
   "source": [
    "### Imitation Learning\n",
    "We assume that the expert policy is given by\n",
    "$$\\pi_\\text{exp}:= \\arg\\max_\\pi \\mathbb{E}_\\pi[{\\theta^*}^T\\phi(Y) + \\lambda \\log \\pi(T|X, U)]$$\n",
    "so that it is soft-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb37b1-a0b4-46dd-923a-988db25815ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
