{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e330b0-f025-4c97-9af1-f8b12b4212f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007b8b79-13f6-4d62-97b6-b355952f524b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GpuDevice(id=0, process_index=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1e0ed-a8df-4180-956d-de51f3618ba8",
   "metadata": {},
   "source": [
    "## Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb1ddd-52f8-47e9-83db-085bcd613c21",
   "metadata": {},
   "source": [
    "#### Formulation in Population\n",
    "\n",
    "We are interested in offline imitation learning with an unknown return function $r(y) \\in \\{\\theta^T\\phi(y): \\theta\\in\\Theta\\}$ for $\\Theta:=\\{\\theta: \\geq 0, \\sum_k\\theta_k=1\\}$ with discrete treatment $t\\in\\mathcal{T}$.\n",
    "The expert policy depends on an unobserved confounder $U$ while the learned poicy does not, so that they are $\\pi_\\text{exp}(t|x, u)$ and $\\pi(t|x)$. Under these assumptions, the imitation learning (i.e. multiobjective policy improvement) from offline dataset becomes:\n",
    "$$\\hat\\pi \n",
    "= \\arg\\max_\\pi\\min_{\\theta\\in\\Theta}[\\min_{w\\in \\mathcal{W}}V_w^\\pi - V^{\\pi_\\text{exp}}].$$\n",
    "Here, the objective is defined as:\n",
    "$$\n",
    "\\min_{w\\in \\mathcal{W}}V_w^\\pi - V^{\\pi_\\text{exp}}\n",
    "= \\min_{w\\in\\mathcal{W}}\n",
    "\\mathbb{E}_X\\mathbb{E}_U\\left[\n",
    "  \\sum_t\\int\\mathrm{d}p(y|t, X, U)\\pi_\\text{exp}(t|X, U)w(t, X, U)\\pi(t|X) \\theta^T\\phi(y) - \\sum_t\\int\\mathrm{d}p(y|t, X, U)\\pi_\\text{exp}(t|X, U) \\theta^T\\phi(y) \n",
    "\\right],\n",
    "$$\n",
    "where $\\mathcal{W}$ is an uncertain set of the inverse treatment probability $\\frac{1}{p(t|X, U)}$ satisfying $\\frac{1}{\\Gamma} \\leq \\frac{(1-\\pi_\\text{exp}(t|x))\\pi_\\text{exp}(t|x, u)}{\\pi_\\text{exp}(t|x)(1-\\pi_\\text{exp}(t|x, u))} \\leq \\Gamma$ for marginalized policy $\\pi_\\text{exp}(t|x) := \\mathbb{E}_U\\pi_\\text{exp}(t|x, U)$ and $\\mathbb{E}\\left[\\mathbb{1}[T=t]w(T, X, U)\\right]=1$ for any $t\\in\\mathcal{T}$. The first condition can be reformulated as \n",
    "$a_{t, x} \\leq w(t, x, u) \\leq b_{t, x}$ with $a_{t, x}:=1 + \\Gamma^{-1} \\cdot \\left(\\frac{1}{\\pi_\\text{exp}(t|x)}-1\\right)$ and $b_{t, x}:=1 + \\Gamma\\cdot\\left(\\frac{1}{\\pi_\\text{exp}(t|x)}-1\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64879eb2-0e36-43c6-9bfe-e8182b53bd1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Re-formulation in Sample \n",
    "Given samples $\\{(X_i, T_i, Y_i) \\}_{i=1}^n$ taken from the expert policy $\\pi_\\text{exp}$, we can approximate the above objective as follows:\n",
    "$$\n",
    "\\min_{w\\in \\mathcal{W}}V_w^\\pi - V^{\\pi_\\text{exp}}\n",
    "\\approx \\min_{w\\in\\mathcal{W}_n}\n",
    "\\frac{1}{n}\\sum_{i=1}^n \\left[\n",
    "  w_i\\pi(t|X) \\theta^T\\phi(y) - \\theta^T\\phi(y) \n",
    "\\right],\n",
    "$$\n",
    "where \n",
    "$$\\mathcal{W}_n:= \\left\\{(w_1, \\ldots, w_n) :\n",
    "a_{T_i, X_i} \\leq w_i \\leq b_{T_i, X_i} \\text{ for any }i=1, \\ldots, n \\text{ and }\\frac{1}{n}\\sum_{i=1}^n\\mathbb{1}[T_i=t]w_i=1 \\text{ for any }t\\in\\mathcal{T}\n",
    "\\right\\}.\n",
    "$$\n",
    "Therefore, our imitation policy is given by\n",
    "$$\n",
    "\\hat \\pi = \n",
    "\\arg\\max_\\pi\\min_{\\theta\\in\\Theta}\\min_{w\\in\\mathcal{W}_n}\n",
    "\\frac{1}{n}\\sum_{i=1}^n \\left[\n",
    "  w_i\\pi(t|X) \\theta^T\\phi(y) - \\theta^T\\phi(y) \n",
    "\\right]\n",
    ".$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81091716-5ac6-477c-9fc0-bc6668c241cc",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934e0ed-e5ca-48fb-b916-4af20f239ea8",
   "metadata": {},
   "source": [
    "## Toy Example\n",
    "We consider the following 2-dimensional model:\n",
    "\\begin{align*}\n",
    "X_i&\\sim N(0, I_2)\n",
    "\\\\\n",
    "U_i&\\sim N(0, I_2)\n",
    "\\\\\n",
    "T_i &\\sim \\pi(t|X_i, U_i)\n",
    "\\\\\n",
    "Y_i&\\sim N(X_i + U_i + T_i, I_2)\n",
    "\\end{align*}\n",
    "where $t =(t_1, t_2)^T \\in \\mathcal{T}:=\\{-1, 0, 1\\}^2$. As for the class of return function, we define $\\phi(y):= (\\|y_1\\|, \\|y_2\\|)^T$ and assume the true parameter is $\\theta^*:=(0, 0)^T$. We assume that the expert policy is given by\n",
    "$$\\pi_\\text{exp}:= \\arg\\max_\\pi \\mathbb{E}_\\pi[{\\theta^*}^T\\phi(Y) + \\lambda \\log \\pi(T|X, U)]$$\n",
    "so that it is soft-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1130d73-4264-494a-a36f-a036507e3be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
